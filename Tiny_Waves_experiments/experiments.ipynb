{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04f894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(globals()[\"_dh\"][0])\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0315897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models_mae_hetero\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from gaussian_noise import AddGaussianNoise\n",
    "from dataset_classes.pretrain_csi_5g import CSI5G\n",
    "from dataset_classes.pretrain_csi_wifi import CSIWiFi\n",
    "from dataset_classes.spectrogram_images import SpectrogramImages\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import util.misc as misc\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "\n",
    "import models_mae_hetero\n",
    "from engine_pretrain_hetero import train_one_epoch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_pruning as tp\n",
    "\n",
    "import timm\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from engine_pretrain_hetero import RoundRobinLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68bde72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model = MaskedAutoencoderViT(\n",
      "  (patch_embed): ModuleList(\n",
      "    (0): PatchEmbed(\n",
      "      (proj): Conv2d(1, 512, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (1): PatchEmbed(\n",
      "      (proj): Conv2d(3, 512, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (2): PatchEmbed(\n",
      "      (proj): Conv2d(4, 512, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_embed): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0-7): 8 x Block(\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (decoder_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_pred): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=768, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = 'mae_vit_small_patch16'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# define the model\n",
    "model = models_mae_hetero.__dict__[model](norm_pix_loss=False, in_chans=[1, 3, 4])\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "print(\"Model = %s\" % str(model_without_ddp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8cac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/ict317-3/Mohammad/Tiny-WFMs/output_dir/best_model.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d18da48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CEViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(2, 512, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_pred): Linear(in_features=256, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148d7f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dataset_classes.spectrogram_images.SpectrogramImages object at 0x7988679ee990> <dataset_classes.pretrain_csi_5g.CSI5G object at 0x798867952bd0> <dataset_classes.pretrain_csi_wifi.CSIWiFi object at 0x79885038efd0>\n"
     ]
    }
   ],
   "source": [
    "augmentation = True\n",
    "\n",
    "data_path = ['/home/ict317-3/Mohammad/Tiny-WFMs/pretraining_datasets/spectrogram_dataset',\n",
    "             '/home/ict317-3/Mohammad/Tiny-WFMs/pretraining_datasets/spectrogram_iqengine_dataset',\n",
    "             '/home/ict317-3/Mohammad/Tiny-WFMs/pretraining_datasets/5G_CFR',\n",
    "             '/home/ict317-3/Mohammad/Tiny-WFMs/pretraining_datasets/NTU-Fi-HumanID']\n",
    "\n",
    "log_dir = './output_dir'\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.functional.pil_to_tensor,\n",
    "        transforms.Lambda(lambda x: 10 * torch.log10(x + 1e-12)),\n",
    "        transforms.Lambda(lambda x: (x + 120) / (-0.5 + 120)),\n",
    "        transforms.Resize((224, 224), antialias=True,\n",
    "                          interpolation=InterpolationMode.BICUBIC),  # Resize\n",
    "        transforms.Normalize(mean=[0.451], std=[0.043])  # Normalize\n",
    "    ])\n",
    "\n",
    "dataset_train_one = SpectrogramImages(data_path[:-2], transform=transform_train)\n",
    "\n",
    "augment_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    AddGaussianNoise(mean=0.0, std=0.05)]\n",
    ")\n",
    "\n",
    "if augmentation:\n",
    "        dataset_train_two = CSI5G(data_path[-2], augment_transforms=augment_transforms)\n",
    "        dataset_train_three = CSIWiFi(data_path[-1], augment_transforms=augment_transforms)\n",
    "else:\n",
    "    dataset_trainlog_dir_two = CSI5G(data_path[-2])\n",
    "    dataset_train_three = CSIWiFi(data_path[-1])\n",
    "\n",
    "print(dataset_train_one, dataset_train_two, dataset_train_three)\n",
    "\n",
    "sampler_train_one = RandomSampler(dataset_train_one)\n",
    "sampler_train_two = RandomSampler(dataset_train_two)\n",
    "sampler_train_three = RandomSampler(dataset_train_three)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe15ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 10\n",
    "pin_mem = True\n",
    "csi_subsampling = False\n",
    "\n",
    "data_loader_train_one = DataLoader(\n",
    "        dataset_train_one, sampler=sampler_train_one,\n",
    "        batch_size= batch_size,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n",
    "\n",
    "if  csi_subsampling:\n",
    "    data_loader_train_two = DataLoader(\n",
    "        dataset_train_two, sampler=sampler_train_two,\n",
    "        batch_size= batch_size // 2,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n",
    "\n",
    "    data_loader_train_three = DataLoader(\n",
    "        dataset_train_three, sampler=sampler_train_three,\n",
    "        batch_size= batch_size // 2,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n",
    "else:\n",
    "    data_loader_train_two = DataLoader(\n",
    "        dataset_train_two, sampler=sampler_train_two,\n",
    "        batch_size= batch_size,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n",
    "\n",
    "    data_loader_train_three = DataLoader(\n",
    "        dataset_train_three, sampler=sampler_train_three,\n",
    "        batch_size= batch_size,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d39d75cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base lr: 1.00e-03\n",
      "actual lr: 6.25e-05\n",
      "accumulate grad iterations: 1\n",
      "effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "accum_iter = 1\n",
    "lr = None\n",
    "blr = 1e-3\n",
    "\n",
    "\n",
    "eff_batch_size =  batch_size *accum_iter\n",
    "    \n",
    "if lr is None:  # only base_lr is specified\n",
    "     lr = blr * eff_batch_size / 256\n",
    "\n",
    "print(\"base lr: %.2e\" % ( lr * 256 / eff_batch_size))\n",
    "print(\"actual lr: %.2e\" %  lr)\n",
    "\n",
    "print(\"accumulate grad iterations: %d\" %accum_iter)\n",
    "print(\"effective batch size: %d\" % eff_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d0239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 6.25e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 6.25e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ict317-3/Mohammad/Tiny-WFMs/util/misc.py:254: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_decay = 0.05\n",
    "\n",
    "# following timm: set wd as 0 for bias and norm layers\n",
    "param_groups = optim_factory.param_groups_weight_decay(model_without_ddp, weight_decay)\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=(0.9, 0.95))\n",
    "print(optimizer)\n",
    "loss_scaler = NativeScaler()\n",
    "\n",
    "ckpt_path = '/home/ict317-3/Mohammad/Tiny-WFMs/checkpoints/pretrained_all_data.pth'\n",
    "pretrained = torch.load(ckpt_path, map_location=device, weights_only=False)['model']\n",
    "model.load_state_dict(pretrained, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82257854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_baseline_performance(model, mask_ratio, kernel_size, dataset_id, device):\n",
    "\n",
    "#     accuracy = 0\n",
    "#     total_loss = 0\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     model = model.to(device)\n",
    "        \n",
    "#     data = {\n",
    "#         1: (dataset_train_one,   data_loader_train_one),\n",
    "#         2: (dataset_train_two,   data_loader_train_two),\n",
    "#         3: (dataset_train_three, data_loader_train_three),\n",
    "#     }\n",
    "#     assert dataset_id in data, f\"dataset_id must be 1, 2, or 3, got {dataset_id}\"\n",
    "\n",
    "\n",
    "#     with torch.no_grad():\n",
    "       \n",
    "#         for k, (images, _) in enumerate(tqdm(data[dataset_id][1], desc=\"Batches\", leave=False)):\n",
    "#             images = images.to(device)\n",
    "#             loss, reconstructed, mask = model(images, mask_ratio=mask_ratio / 100)\n",
    "#             images = torch.einsum('nchw->nhwc', images)\n",
    "#             reconstructed = torch.einsum('nchw->nhwc', model.unpatchify(reconstructed))\n",
    "#             mask = model.unpatchify(mask.unsqueeze(-1).repeat(1, 1, 16 ** 2 * 1))\n",
    "#             mask = torch.einsum('nchw->nhwc', mask)\n",
    "#             reconstructed = (1 - mask) * images + mask * reconstructed\n",
    "\n",
    "#             pooled_images = F.avg_pool2d(images.permute(0, 3, 1, 2), kernel_size=kernel_size, stride=kernel_size)\n",
    "#             pooled_reconstructed = F.avg_pool2d(reconstructed.permute(0, 3, 1, 2), kernel_size=kernel_size, stride=kernel_size)\n",
    "#             mu, std = torch.mean(pooled_images, dim=(1, 2, 3)), torch.std(pooled_images, dim=(1, 2, 3))\n",
    "#             threshold = mu + 0.5 * std\n",
    "#             threshold = threshold.view(-1, 1, 1, 1).repeat((1, 1, pooled_images.shape[2], pooled_images.shape[3]))\n",
    "#             pooled_images = pooled_images > threshold\n",
    "#             pooled_reconstructed = pooled_reconstructed > threshold\n",
    "\n",
    "#             accuracy += (pooled_images == pooled_reconstructed).sum().item()\n",
    "#             total_loss += loss.item() \n",
    "\n",
    "#     accuracy /= (len(data[dataset_id][0]) * (224 // kernel_size) ** 2)\n",
    "#     avg_loss = total_loss / len(data[dataset_id][1])\n",
    "\n",
    "#     print(accuracy)\n",
    "#     return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "158300f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_per_loader(model, device, data_loader, mask_ratio=75, kernel_size=3):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # map id -> (dataset, loader)\n",
    "    # data = {\n",
    "    #     1: data_loader_train_one,\n",
    "    #     2: data_loader_train_two,\n",
    "    #     3: data_loader_train_three,\n",
    "    # }\n",
    "    # assert dataset_id in data, f\"dataset_id must be 1, 2, or 3, got {dataset_id}\"\n",
    "\n",
    "    # loader = data[dataset_id]\n",
    "\n",
    "    total_equal = 0\n",
    "    total_cells_seen = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Try to infer patch size and channels for robust unpatchify of the mask\n",
    "    # Fallback to 16 and 1 if the model doesn't expose it\n",
    "    try:\n",
    "        p = getattr(model, \"patch_embed\").patch_size[0]  # e.g., 16\n",
    "    except Exception:\n",
    "        p = 16\n",
    "    # We'll infer channels per batch from the images\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(data_loader, desc=\"Batches\", leave=False):\n",
    "            images = images.to(device)  # NCHW expected here from your dataset pipeline\n",
    "\n",
    "            # forward\n",
    "            loss, reconstructed, mask = model(images, mask_ratio=mask_ratio / 100.0)\n",
    "\n",
    "            # Unpatchify outputs (assumes MAE-like shapes)\n",
    "            # reconstructed: (N, L, p*p*C), mask: (N, L)\n",
    "            # Infer channels from the reconstructed payload if possible\n",
    "            # C = (reconstructed.shape[-1] // (p * p)) if reconstructed.ndim == 3 else images.shape[1]\n",
    "            if reconstructed.ndim == 3:\n",
    "                C = reconstructed.shape[-1] // (p * p)\n",
    "            else:\n",
    "                C = images.shape[1]\n",
    "\n",
    "            reconstructed = model.unpatchify(reconstructed)  # -> (N, C, H, W)\n",
    "            # Broadcast mask from (N, L) to (N, p*p*C, L) before unpatchify\n",
    "            mask_expanded = mask.unsqueeze(-1).repeat(1, 1, p * p * C)\n",
    "            mask_img = model.unpatchify(mask_expanded)  # (N, C, H, W)\n",
    "\n",
    "            # Blend original and reconstructed only on masked regions\n",
    "            blended = (1 - mask_img) * images + mask_img * reconstructed  # all NCHW\n",
    "\n",
    "            # Pool down to grids\n",
    "            pooled_images = F.avg_pool2d(images, kernel_size=kernel_size, stride=kernel_size)\n",
    "            pooled_blended = F.avg_pool2d(blended, kernel_size=kernel_size, stride=kernel_size)\n",
    "\n",
    "            # Per-sample thresholds from original pooled images\n",
    "            # mean/std over (C,H,W)\n",
    "            mu = pooled_images.mean(dim=(1, 2, 3))\n",
    "            std = pooled_images.std(dim=(1, 2, 3), unbiased=False)  # avoid NaNs when only 1 cell\n",
    "            threshold = (mu + 0.5 * std).view(-1, 1, 1, 1)\n",
    "\n",
    "            # Binarize\n",
    "            bin_images = pooled_images > threshold\n",
    "            bin_blended = pooled_blended > threshold\n",
    "\n",
    "            # Count equal cells\n",
    "            equal_cells = (bin_images == bin_blended).sum().item()\n",
    "            total_equal += equal_cells\n",
    "\n",
    "            # Track how many cells we actually processed\n",
    "            n, c, h, w = bin_images.shape\n",
    "            total_cells_seen += n * c * h * w\n",
    "\n",
    "            # Accumulate batch loss\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "    # Safe aggregations\n",
    "    accuracy = total_equal / max(1, total_cells_seen)\n",
    "    avg_loss = total_loss / max(1, len(data_loader))\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aadf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_ratio = 75\n",
    "# kernel_size = 3\n",
    "# dataset_id = 3\n",
    "\n",
    "\n",
    "# loss = compute_loss_per_loader(model, data_loader_train_one, device)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb65f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baseline_performance(model, device, data_loaders):\n",
    "\n",
    "    overall_loss = sum(compute_loss_per_loader(model, device, loader)[0] for loader in data_loaders) / len(data_loaders)\n",
    "    \n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "502c5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_encoder_new(self, x, mask_ratio):\n",
    "#         # embed patches\n",
    "#         x = self.patch_embed(x)\n",
    "\n",
    "#         # add pos embed w/o cls token\n",
    "#         x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "#         # masking: length -> length * mask_ratio\n",
    "#         x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "#         # append cls token\n",
    "#         cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "#         cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "#         # apply Transformer blocks\n",
    "#         for blk in self.blocks:\n",
    "#             x = blk(x)\n",
    "#         x = self.norm(x)\n",
    "\n",
    "#         return x, mask, ids_restore\n",
    "\n",
    "\n",
    "# def forward_decoder_new(self, x, ids_restore):\n",
    "#         # embed tokens\n",
    "#         x = self.decoder_embed(x)\n",
    "\n",
    "#         # append mask tokens to sequence\n",
    "#         mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "#         x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "#         x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "#         x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "#         # add pos embed\n",
    "#         x = x + self.decoder_pos_embed\n",
    "\n",
    "#         # apply Transformer blocks\n",
    "#         for blk in self.decoder_blocks:\n",
    "#             x = blk(x)\n",
    "#         x = self.decoder_norm(x)\n",
    "\n",
    "#         # predictor projection\n",
    "#         x = self.decoder_pred(x)\n",
    "\n",
    "#         # remove cls token\n",
    "#         x = x[:, 1:, :]\n",
    "\n",
    "#         return x\n",
    "\n",
    "# def forward_new(self, imgs, mask_ratio=0.75):\n",
    "#     latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)  # <- renamed\n",
    "#     pred = self.forward_decoder(latent, ids_restore)                    # <- renamed\n",
    "#     loss = self.forward_loss(imgs, pred, mask)\n",
    "#     return loss, pred, mask\n",
    "\n",
    "# import types\n",
    "\n",
    "# model.forward_encoder = types.MethodType(forward_encoder_new, model)\n",
    "# model.forward_decoder = types.MethodType(forward_decoder_new, model)\n",
    "# model.forward          = types.MethodType(forward_new, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b801091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loaders = [data_loader_train_one, data_loader_train_two, data_loader_train_three]\n",
    "\n",
    "# loss = compute_baseline_performance(model, 'cuda', data_loaders)\n",
    "\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07cf49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocks(model):\n",
    "    ignored_blocks = []\n",
    "    model_blocks = []\n",
    "    num_heads = {}\n",
    "    bottleneck = False\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, timm.models.vision_transformer.Attention):\n",
    "            num_heads[m.qkv] = m.num_heads\n",
    "        if bottleneck and isinstance(m, timm.models.vision_transformer.Mlp):\n",
    "            ignored_blocks.append(m.fc2)\n",
    "\n",
    "    for name, layer in model.named_children():\n",
    "        if name != 'blocks':\n",
    "            if len(list(layer.children())) > 0:\n",
    "                ignored_blocks.extend(layer.children())\n",
    "            else:\n",
    "                ignored_blocks.append(layer)\n",
    "        else:\n",
    "            if len(list(layer.children())) > 0:\n",
    "                model_blocks.extend(layer.children())\n",
    "            else:\n",
    "                model_blocks.append(layer)\n",
    "\n",
    "    return model_blocks, ignored_blocks, num_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ba79728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_block_pruning(trained_model, device, pruning_ratios, prune_method, data_loaders, mask_ratio):\n",
    "    model = copy.deepcopy(trained_model)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model_blocks, ignored_blocks, num_heads = get_blocks(model)\n",
    "    model.to(device)\n",
    "\n",
    "    pruning_info = {\n",
    "        i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio}\n",
    "        for i, ratio in enumerate(pruning_ratios)\n",
    "    }\n",
    "\n",
    "    if prune_method == 'channel_pruning_Taylor_importance':\n",
    "        imp = tp.importance.GroupTaylorImportance()\n",
    "\n",
    "        combined_iter = RoundRobinLoader(data_loaders)\n",
    "\n",
    "        if isinstance(imp, tp.importance.GroupTaylorImportance):\n",
    "            model.zero_grad()\n",
    "            model.train(True)\n",
    "\n",
    "            print(\"Accumulating gradients for pruning...\")\n",
    "            for data_iter_step, (key, samples, labels) in enumerate(combined_iter):\n",
    "                samples, labels = samples.to(device), labels.to(device)\n",
    "                if data_iter_step >= 60: # 20 samples of each dataloader for now\n",
    "                    break\n",
    "                loss, _, _ = model(samples, mask_ratio=mask_ratio / 100)\n",
    "                loss.backward()\n",
    "\n",
    "        original_macs, original_nparams = tp.utils.count_ops_and_params(model, samples)\n",
    "\n",
    "        for i, info in pruning_info.items():\n",
    "            pruning_ratio = info[\"pruning_ratio\"]\n",
    "            if pruning_ratio == 0:\n",
    "                continue\n",
    "\n",
    "            ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "            combined_ignored_layers = ignored_blocks + ignored_layers_block\n",
    "            # print('combined_ignored_layers:   ', combined_ignored_layers)\n",
    "            \n",
    "            count = 0\n",
    "            while True:\n",
    "                pruner = tp.pruner.MetaPruner(\n",
    "                    model,\n",
    "                    example_inputs=samples,\n",
    "                    importance=imp,\n",
    "                    pruning_ratio=pruning_ratio,\n",
    "                    ignored_layers=combined_ignored_layers,\n",
    "                    num_heads=num_heads,\n",
    "                    prune_num_heads=False,\n",
    "                    prune_head_dims=True\n",
    "                )\n",
    "                for g in pruner.step(interactive=True):\n",
    "                    g.prune()\n",
    "\n",
    "                # print(f'A pruning process has been performed here with a {pruning_ratio} pruning ratio ...  ')\n",
    "                \n",
    "                for m in model.modules():\n",
    "                    if isinstance(m, timm.models.vision_transformer.Attention):\n",
    "                        m.num_heads = pruner.num_heads[m.qkv]\n",
    "                        m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n",
    "\n",
    "                macs, nparams = tp.utils.count_ops_and_params(model, samples)\n",
    "\n",
    "                if original_nparams - nparams == 0:\n",
    "                    count += 1\n",
    "                    if count == 1:\n",
    "                        pruning_ratio = 0.5\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                original_nparams = nparams\n",
    "\n",
    "        del samples, labels\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return model, macs, nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7a3a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_analysis_with_contributions(original_model, device, metric, measure_performance, data_loaders, mask_ratio=75):\n",
    "    model_blocks, _, _ = get_blocks(original_model)\n",
    "    blocks_number = len(model_blocks)\n",
    "\n",
    "    total_block_performance = [0.0 for _ in range(blocks_number)]\n",
    "    params_reduction = []\n",
    "    macs_reduction = []\n",
    "\n",
    "    original_model.to(device)\n",
    "\n",
    "    print(f\"Computing baseline {metric} without block replacement...\")\n",
    "    baseline_performance = measure_performance(original_model, device, data_loaders)\n",
    "    print(f\"Baseline {metric}: {baseline_performance}\")\n",
    "\n",
    "    example_inputs = next(iter(data_loaders[0]))[0].to(device)\n",
    "    original_macs, original_nparams = tp.utils.count_ops_and_params(original_model, example_inputs)\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "        print(f\"Replacing block {block_idx}\")\n",
    "        pruning_ratios = (np.eye(blocks_number) * 0.85)[block_idx]\n",
    "        pruned_model, macs, nparams = selective_block_pruning(\n",
    "            original_model, device, pruning_ratios, 'channel_pruning_Taylor_importance', data_loaders, mask_ratio\n",
    "        )\n",
    "\n",
    "        params_reduction.append((original_nparams - nparams)/original_nparams * 100)\n",
    "        macs_reduction.append((original_macs - macs) / original_macs * 100)\n",
    "\n",
    "        pruned_model.to(device)\n",
    "        block_performance = measure_performance(pruned_model, device, data_loaders)\n",
    "        total_block_performance[block_idx] = block_performance\n",
    "        print(f'The {metric} after pruning this block is: {block_performance}')\n",
    "\n",
    "    total_degradation_in_performance = 0.0\n",
    "    block_degradation = []\n",
    "    total_params_reduction = 0.0\n",
    "    total_macs_reduction = 0.0\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "        degradation = np.abs(total_block_performance[block_idx] - baseline_performance)\n",
    "        print(f\"Degradation in {metric} is: {degradation}\")\n",
    "        block_degradation.append(degradation)\n",
    "        total_degradation_in_performance += degradation\n",
    "        total_params_reduction += params_reduction[block_idx]\n",
    "        total_macs_reduction += macs_reduction[block_idx]\n",
    "\n",
    "    relative_contributions = []\n",
    "    weighted_importance_scores = []\n",
    "\n",
    "    print(f\"\\nRelative contribution of each block to total {metric} degradation and parameter reduction:\")\n",
    "    for block_idx in range(blocks_number):\n",
    "        rel_perf = (block_degradation[block_idx] / total_degradation_in_performance) * 100\n",
    "        rel_params = 100 - params_reduction[block_idx]\n",
    "        rel_macs = 100 - macs_reduction[block_idx]\n",
    "\n",
    "        weighted_importance = (0.7 * rel_perf) + (0.2 * rel_params) + (0.1 * rel_macs)\n",
    "        print(f'Block {block_idx} contributes {rel_perf:.2f}% to the total degradation in {metric} and reduces {params_reduction[block_idx]:.2f}% of parameters.')\n",
    "        print(f'Weighted importance score for Block {block_idx}: {weighted_importance:.2f}')\n",
    "\n",
    "        relative_contributions.append(rel_perf)\n",
    "        weighted_importance_scores.append(weighted_importance)\n",
    "\n",
    "    return weighted_importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378c846",
   "metadata": {},
   "source": [
    "## Performing the compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a31d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    \"\"\"https://github.com/huggingface/pytorch-image-models/blob/054c763fcaa7d241564439ae05fbe919ed85e614/timm/models/vision_transformer.py#L79\"\"\"\n",
    "    B, N, C = x.shape\n",
    "    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv.unbind(0)\n",
    "    q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "    if self.fused_attn:\n",
    "        x = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            dropout_p=self.attn_drop.p,\n",
    "        )\n",
    "    else:\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = attn @ v\n",
    "\n",
    "    x = x.transpose(1, 2).reshape(B, N, -1) # original implementation: x = x.transpose(1, 2).reshape(B, N, C)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "    return x\n",
    "\n",
    "# Override attention forward\n",
    "for m in model.modules():\n",
    "    if isinstance(m, timm.models.vision_transformer.Attention):\n",
    "        m.forward = forward.__get__(m, timm.models.vision_transformer.Attention)\n",
    "        \n",
    "# Analyze contributions\n",
    "# relative_contribution = perplexity_analysis_with_contributions(\n",
    "#     model,\n",
    "#     device=device,\n",
    "#     metric='loss',\n",
    "#     measure_performance=compute_baseline_performance,\n",
    "#     data_loaders=[data_loader_train_one, data_loader_train_two, data_loader_train_three]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04022fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relative_contribution = [np.float64(68.08877057598382),\n",
    " np.float64(37.40626909822759),\n",
    " np.float64(32.281119143100256),\n",
    " np.float64(30.637165097452176),\n",
    " np.float64(30.020183495139563),\n",
    " np.float64(29.474453270385183),\n",
    " np.float64(29.440264146737952),\n",
    " np.float64(29.357529826182528),\n",
    " np.float64(29.954904338606084),\n",
    " np.float64(29.66567846836903),\n",
    " np.float64(30.0643648149045),\n",
    " np.float64(31.607907928914102)]\n",
    "\n",
    "x = np.arange(len(relative_contribution))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(x, relative_contribution, color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.xlabel(\"Blocks\", fontsize=12)\n",
    "plt.ylabel(\"Relative contribution\", fontsize=12)\n",
    "plt.title(\"Blocks contribution to performance\", fontsize=14)\n",
    "plt.xticks(x)  # show indices as x-axis labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd73c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pruning_ratios(contributions, max_pruning_ratio=0.9, k=5):\n",
    "    \"\"\"\n",
    "    Calculate pruning ratios based on intense nonlinear scaling (exponential decay) of the relative contributions.\n",
    "\n",
    "    Parameters:\n",
    "    - contributions (list): List of relative contributions (in percentages) of each block to total loss increase.\n",
    "    - max_pruning_ratio (float): Maximum pruning ratio to be assigned to the least important layer. Default is 0.9.\n",
    "    - k (int): Factor controlling the intensity of the scaling (larger k makes the ratio more intense).\n",
    "\n",
    "    Returns:\n",
    "    - pruning_ratios (list): List of pruning ratios for each block.\n",
    "    \"\"\"\n",
    "    # Normalize the contributions to get values between 0 and 1\n",
    "    total_contribution = sum(contributions)\n",
    "    normalized_contributions = [contribution / total_contribution for contribution in contributions]\n",
    "\n",
    "    # Apply exponential decay to magnify the effect for less important blocks\n",
    "    pruning_factors = [np.exp(-k * nc) for nc in normalized_contributions]\n",
    "\n",
    "    # Normalize the pruning factors so they stay within the max pruning ratio\n",
    "    max_factor = max(pruning_factors)\n",
    "    normalized_factors = [pf / max_factor for pf in pruning_factors]\n",
    "\n",
    "    # Scale by the maximum pruning ratio\n",
    "    pruning_ratios = [max_pruning_ratio * nf for nf in normalized_factors]\n",
    "\n",
    "    pruning_ratios = [round(num, 2) for num in pruning_ratios]\n",
    "\n",
    "    return pruning_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e69ce19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(trained_model, device, pruning_ratios, prune_method, data_loaders, mask_ratio=75):\n",
    "    model = copy.deepcopy(trained_model)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model_blocks, ignored_blocks, num_heads = get_blocks(model)\n",
    "    model.to(device)\n",
    "\n",
    "    pruning_info = {\n",
    "        i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio}\n",
    "        for i, ratio in enumerate(pruning_ratios)\n",
    "    }\n",
    "\n",
    "    if prune_method == 'channel_pruning_Taylor_importance':\n",
    "        imp = tp.importance.GroupTaylorImportance()\n",
    "\n",
    "        combined_iter = RoundRobinLoader(data_loaders)\n",
    "\n",
    "        if isinstance(imp, tp.importance.GroupTaylorImportance):\n",
    "            model.zero_grad()\n",
    "            model.train(True)\n",
    "\n",
    "            print(\"Accumulating gradients for pruning...\")\n",
    "            for data_iter_step, (key, samples, labels) in enumerate(combined_iter):\n",
    "                samples, labels = samples.to(device), labels.to(device)\n",
    "                if data_iter_step >= 60: # 20 samples of each dataloader for now\n",
    "                    break\n",
    "                loss, _, _ = model(samples, mask_ratio=mask_ratio / 100)\n",
    "                loss.backward()\n",
    "\n",
    "        original_macs, original_nparams = tp.utils.count_ops_and_params(model, samples)\n",
    "\n",
    "        for i, info in pruning_info.items():\n",
    "            pruning_ratio = info[\"pruning_ratio\"]\n",
    "   \n",
    "\n",
    "            ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "            combined_ignored_layers = ignored_blocks + ignored_layers_block\n",
    "            # print('combined_ignored_layers:   ', combined_ignored_layers)\n",
    "            \n",
    "            print(f\"Pruning block {i} with pruning ratio: {pruning_ratio}\")\n",
    "\n",
    "            pruner = tp.pruner.MetaPruner(\n",
    "                model,\n",
    "                example_inputs=samples,\n",
    "                importance=imp,\n",
    "                pruning_ratio=pruning_ratio,\n",
    "                ignored_layers=combined_ignored_layers,\n",
    "                num_heads=num_heads,\n",
    "                prune_num_heads=False,\n",
    "                prune_head_dims=True\n",
    "            )\n",
    "            for g in pruner.step(interactive=True):\n",
    "                g.prune()\n",
    "\n",
    "            # print(f'A pruning process has been performed here with a {pruning_ratio} pruning ratio ...  ')\n",
    "            \n",
    "            for m in model.modules():\n",
    "                if isinstance(m, timm.models.vision_transformer.Attention):\n",
    "                    m.num_heads = pruner.num_heads[m.qkv]\n",
    "                    m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n",
    "\n",
    "            macs, nparams = tp.utils.count_ops_and_params(model, samples)\n",
    "\n",
    "            print(f\"MACs: {macs / 1e9:.2f} G, #Params: {nparams / 1e3:.2f} K\")\n",
    "            print(f\"Parameter reduction: {((original_nparams - nparams) / original_macs * 100):.2f}%\")\n",
    "            print(f\"MACs reduction: {((original_macs - macs) / original_macs * 100):.2f}%\")\n",
    "        \n",
    "\n",
    "                \n",
    "        del samples, labels\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return model, macs, nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5acb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 Pruning Ratio: 0.5000\n",
      "Block 1 Pruning Ratio: 0.5400\n",
      "Block 2 Pruning Ratio: 0.5500\n",
      "Block 3 Pruning Ratio: 0.5500\n",
      "Block 4 Pruning Ratio: 0.5500\n",
      "Block 5 Pruning Ratio: 0.5500\n",
      "Block 6 Pruning Ratio: 0.5500\n",
      "Block 7 Pruning Ratio: 0.5500\n",
      "Block 8 Pruning Ratio: 0.5500\n",
      "Block 9 Pruning Ratio: 0.5500\n",
      "Block 10 Pruning Ratio: 0.5500\n",
      "Block 11 Pruning Ratio: 0.5500\n"
     ]
    }
   ],
   "source": [
    "max_pruning_ratio = 0.9 # Maximum pruning ratio (99%) \n",
    "k = 5 # Controls the intensity of the scaling\n",
    "\n",
    "pruning_ratios = calculate_pruning_ratios(relative_contribution, max_pruning_ratio, k)\n",
    "\n",
    "# Print the pruning ratios for each block\n",
    "for i, ratio in enumerate(pruning_ratios):\n",
    "    print(f\"Block {i} Pruning Ratio: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64639093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating gradients for pruning...\n",
      "Pruning block 0 with pruning ratio: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ict317-3/Mohammad/Tiny-WFMs/wavenv/lib/python3.11/site-packages/torch_pruning/dependency.py:699: UserWarning: Unwrapped parameters detected: ['pos_embed', 'decoder_pos_embed', 'mask_token', 'cls_token'].\n",
      " Torch-Pruning will prune the last non-singleton dimension of these parameters. If you wish to change this behavior, please provide an unwrapped_parameters argument.\n",
      "  warnings.warn(warning_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs: 3.29 G, #Params: 44433.41 K\n",
      "Parameter reduction: 0.05%\n",
      "MACs reduction: 2.47%\n",
      "Pruning block 1 with pruning ratio: 0.54\n",
      "MACs: 3.20 G, #Params: 42725.48 K\n",
      "Parameter reduction: 0.10%\n",
      "MACs reduction: 5.14%\n",
      "Pruning block 2 with pruning ratio: 0.55\n",
      "MACs: 3.11 G, #Params: 40979.61 K\n",
      "Parameter reduction: 0.15%\n",
      "MACs reduction: 7.86%\n",
      "Pruning block 3 with pruning ratio: 0.55\n",
      "MACs: 3.02 G, #Params: 39233.75 K\n",
      "Parameter reduction: 0.20%\n",
      "MACs reduction: 10.58%\n",
      "Pruning block 4 with pruning ratio: 0.55\n",
      "MACs: 2.93 G, #Params: 37487.89 K\n",
      "Parameter reduction: 0.25%\n",
      "MACs reduction: 13.31%\n",
      "Pruning block 5 with pruning ratio: 0.55\n",
      "MACs: 2.84 G, #Params: 35742.03 K\n",
      "Parameter reduction: 0.30%\n",
      "MACs reduction: 16.03%\n",
      "Pruning block 6 with pruning ratio: 0.55\n",
      "MACs: 2.74 G, #Params: 33996.16 K\n",
      "Parameter reduction: 0.36%\n",
      "MACs reduction: 18.76%\n",
      "Pruning block 7 with pruning ratio: 0.55\n",
      "MACs: 2.65 G, #Params: 32250.30 K\n",
      "Parameter reduction: 0.41%\n",
      "MACs reduction: 21.48%\n",
      "Pruning block 8 with pruning ratio: 0.55\n",
      "MACs: 2.56 G, #Params: 30504.44 K\n",
      "Parameter reduction: 0.46%\n",
      "MACs reduction: 24.21%\n",
      "Pruning block 9 with pruning ratio: 0.55\n",
      "MACs: 2.47 G, #Params: 28758.57 K\n",
      "Parameter reduction: 0.51%\n",
      "MACs reduction: 26.93%\n",
      "Pruning block 10 with pruning ratio: 0.55\n",
      "MACs: 2.38 G, #Params: 27012.71 K\n",
      "Parameter reduction: 0.56%\n",
      "MACs reduction: 29.66%\n",
      "Pruning block 11 with pruning ratio: 0.55\n",
      "MACs: 2.28 G, #Params: 25266.85 K\n",
      "Parameter reduction: 0.61%\n",
      "MACs reduction: 32.38%\n"
     ]
    }
   ],
   "source": [
    "data_loaders = [data_loader_train_one, data_loader_train_two, data_loader_train_three]\n",
    "pruned_model, macs, nparams = prune_model(model, device, pruning_ratios, 'channel_pruning_Taylor_importance', data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4866cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs: 3.376390688 G -> 2.4201433056875 G, #Params: 46.008064 M -> 27.959995 M\n",
      "Overall parameter reduction: 39.23%\n",
      "Overall MACs reduction: 28.32%\n"
     ]
    }
   ],
   "source": [
    "images = next(iter(data_loaders[0]))[0].to(device)\n",
    "model.to(device)\n",
    "\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, images)\n",
    "macs, nparams = tp.utils.count_ops_and_params(pruned_model, images)\n",
    "\n",
    "print(f\"MACs: {base_macs/1e9} G -> {macs/1e9} G, #Params: {base_nparams/1e6} M -> {nparams/1e6} M\")\n",
    "p_ratio = ((base_nparams - nparams) / base_nparams * 100)\n",
    "\n",
    "print(f\"Overall parameter reduction: {(p_ratio):.2f}%\")\n",
    "print(f\"Overall MACs reduction: {((base_macs - macs) / base_macs * 100):.2f}%\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3306634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model, f'our_pruned_models/pruned_autoencoder/Vit_pruned_{(p_ratio):.2f}%.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059f5ea",
   "metadata": {},
   "source": [
    "## Applying pruning on only the first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_encoder_params(model, exclude_prefixes=(\"decoder\",), include_buffers=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Counts parameters for everything except the decoder part.\n",
    "    Any top-level child whose name starts with one of `exclude_prefixes`\n",
    "    (e.g., 'decoder', 'decoder_') is excluded along with all its descendants.\n",
    "\n",
    "    Works with models whose children include:\n",
    "      ['patch_embed', 'blocks', 'norm', 'decoder_embed', 'decoder_blocks', 'decoder_norm', 'decoder_pred']\n",
    "\n",
    "    Args:\n",
    "        model: nn.Module\n",
    "        exclude_prefixes: tuple of prefixes to exclude at top level (default: (\"decoder\",))\n",
    "        include_buffers: if True, also include buffers (e.g., running stats) in the total\n",
    "        verbose: print a human summary\n",
    "\n",
    "    Returns:\n",
    "        dict with total, trainable, non_trainable, ratio (2 decimals)\n",
    "    \"\"\"\n",
    "    # Collect which top-level children are considered decoder\n",
    "    decoder_roots = {name for name, _ in model.named_children()\n",
    "                     if any(name.startswith(pref) for pref in exclude_prefixes)}\n",
    "\n",
    "    total = 0\n",
    "    trainable = 0\n",
    "\n",
    "    # Parameters: keep only those whose top-level token is NOT a decoder root\n",
    "    for name, p in model.named_parameters():\n",
    "        root = name.split('.', 1)[0]\n",
    "        if root in decoder_roots or any(root.startswith(pref) for pref in exclude_prefixes):\n",
    "            continue\n",
    "        n = p.numel()\n",
    "        total += n\n",
    "        if p.requires_grad:\n",
    "            trainable += n\n",
    "\n",
    "    # Optionally add buffers (doesn't change \"trainable\", but affects total/non_trainable)\n",
    "    if include_buffers:\n",
    "        for name, b in model.named_buffers():\n",
    "            root = name.split('.', 1)[0]\n",
    "            if root in decoder_roots or any(root.startswith(pref) for pref in exclude_prefixes):\n",
    "                continue\n",
    "            total += b.numel()\n",
    "\n",
    "    non_trainable = total - trainable\n",
    "    ratio = (trainable / total) if total else float(\"nan\")\n",
    "\n",
    "    if verbose:\n",
    "        def human(n): return f\"{n:,} ({n/1e6:.2f}M)\"\n",
    "        print(\"[ENCODER ONLY]\")\n",
    "        print(f\"Total params        : {human(total)}\")\n",
    "        print(f\"Trainable params    : {human(trainable)}\")\n",
    "        print(f\"Non-trainable params: {human(non_trainable)}\")\n",
    "        print(f\"Trainable/Total     : {ratio:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"trainable\": trainable,\n",
    "        \"non_trainable\": non_trainable,\n",
    "        \"ratio\": float(f\"{ratio:.2f}\") if ratio == ratio else ratio,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "max_pruning_ratio = 0.9 # Maximum pruning ratio (99%) \n",
    "k = 0 # Controls the intensity of the scaling\n",
    "\n",
    "pruning_ratios = calculate_pruning_ratios(relative_contribution, max_pruning_ratio, k)\n",
    "\n",
    "# Print the pruning ratios for each block\n",
    "for i, ratio in enumerate(pruning_ratios):\n",
    "    print(f\"Block {i} Pruning Ratio: {ratio:.4f}\")\n",
    "\n",
    "data_loaders = [data_loader_train_one, data_loader_train_two, data_loader_train_three]\n",
    "pruned_model, macs, nparams = prune_model(model, device, pruning_ratios, 'channel_pruning_Taylor_importance', data_loaders)\n",
    "\n",
    "encoder = copy.deepcopy(model)\n",
    "encoder.blocks = torch.nn.Sequential(encoder.blocks[0])\n",
    "\n",
    "pruned_encoder = copy.deepcopy(pruned_model)\n",
    "pruned_encoder.blocks = torch.nn.Sequential(pruned_encoder.blocks[0])\n",
    "\n",
    "\n",
    "encoder_stats = count_encoder_params(encoder)\n",
    "pruned_stats = count_encoder_params(pruned_encoder)\n",
    "\n",
    "base_nparams = encoder_stats['total']\n",
    "nparams = pruned_stats['total']\n",
    "\n",
    "print(f\"#Params: {base_nparams/1e6} M -> {nparams/1e6} M\")\n",
    "p_ratio = ((base_nparams - nparams) / base_nparams * 100)\n",
    "\n",
    "print(f\"Overall parameter reduction: {(p_ratio):.2f}%\")\n",
    "\n",
    "# torch.save(pruned_model, f'our_pruned_models/one_block_ViTs/Vit_pruned_{(p_ratio):.2f}%.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c4c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in model_a but missing in model_b: {'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.7.norm1.weight', 'decoder_pred.0.weight', 'decoder_embed.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.5.mlp.fc1.bias', 'patch_embed.2.proj.bias', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_embed.weight', 'decoder_pos_embed', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.norm2.weight', 'patch_embed.1.proj.bias', 'decoder_pred.1.weight', 'decoder_pred.2.weight', 'decoder_norm.bias', 'patch_embed.2.proj.weight', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.6.norm1.bias', 'patch_embed.0.proj.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.0.norm2.weight', 'mask_token', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.0.mlp.fc1.weight', 'patch_embed.0.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.7.attn.proj.bias', 'decoder_pred.2.bias', 'decoder_blocks.0.attn.proj.weight', 'patch_embed.1.proj.weight', 'decoder_norm.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.3.norm1.weight', 'decoder_pred.1.bias', 'decoder_blocks.2.attn.qkv.bias', 'decoder_pred.0.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.5.mlp.fc2.bias'}\n",
      "Parameters in model_b but missing in model_a: {'head.3.bias', 'patch_embed.proj.bias', 'head.3.weight', 'head.0.bias', 'patch_embed.proj.weight', 'head.0.weight'}\n",
      "[Value mismatch] pos_embed | Max diff: 1.08e+00\n",
      "[Value mismatch] cls_token | Max diff: 6.78e-02\n",
      "[Value mismatch] norm.weight | Max diff: 6.71e-01\n",
      "[Value mismatch] norm.bias | Max diff: 1.37e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pos_embed', 1.0822968482971191),\n",
       " ('cls_token', 0.06781171262264252),\n",
       " ('norm.weight', 0.6712888479232788),\n",
       " ('norm.bias', 0.13735242187976837)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_model_weights(model_a, model_b, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare the weights of two PyTorch models.\n",
    "\n",
    "    Args:\n",
    "        model_a (torch.nn.Module): First model.\n",
    "        model_b (torch.nn.Module): Second model.\n",
    "        verbose (bool): If True, prints differences.\n",
    "\n",
    "    Returns:\n",
    "        differences (list): List of tuples (param_name, max_diff) where differences were found.\n",
    "    \"\"\"\n",
    "    differences = []\n",
    "\n",
    "    state_dict_a = model_a.state_dict()\n",
    "    state_dict_b = model_b.state_dict()\n",
    "\n",
    "    keys_a = set(state_dict_a.keys())\n",
    "    keys_b = set(state_dict_b.keys())\n",
    "\n",
    "    # Check for missing parameters\n",
    "    if keys_a != keys_b:\n",
    "        missing_in_b = keys_a - keys_b\n",
    "        missing_in_a = keys_b - keys_a\n",
    "\n",
    "        if missing_in_b:\n",
    "            print(f\"Parameters in model_a but missing in model_b: {missing_in_b}\")\n",
    "        if missing_in_a:\n",
    "            print(f\"Parameters in model_b but missing in model_a: {missing_in_a}\")\n",
    "\n",
    "        # Only compare common keys\n",
    "        common_keys = keys_a.intersection(keys_b)\n",
    "    else:\n",
    "        common_keys = keys_a\n",
    "\n",
    "    # Compare weights\n",
    "    for key in common_keys:\n",
    "        tensor_a = state_dict_a[key]\n",
    "        tensor_b = state_dict_b[key]\n",
    "\n",
    "        if not torch.allclose(tensor_a, tensor_b, atol=1e-6):\n",
    "            max_diff = (tensor_a - tensor_b).abs().max().item()\n",
    "            differences.append((key, max_diff))\n",
    "            if verbose:\n",
    "                print(f\"[Value mismatch] {key} | Max diff: {max_diff:.2e}\")\n",
    "\n",
    "    if not differences and verbose:\n",
    "        print(\"✅ Models are identical (within tolerance)!\")\n",
    "\n",
    "    return differences\n",
    "\n",
    "\n",
    "original_model = torch.load('/home/ict317-3/Mohammad/Tiny-WFMs/our_pruned_models/pruned_autoencoder/Vit_pruned_68.53%.pth', weights_only=False)\n",
    "pruned_model = torch.load('/home/ict317-3/Mohammad/Tiny-WFMs/pruned_results/sig_identification/best_model.pth', weights_only=False)\n",
    "\n",
    "original_model.to('cuda')\n",
    "pruned_model.to('cuda')\n",
    "compare_model_weights(original_model, pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "\n",
    "    # model.unfreeze_patch_embed()\n",
    "    for param in model.blocks.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Freeze positional embeddings and tokens\n",
    "    if hasattr(model, \"cls_token\"):\n",
    "        model.cls_token.requires_grad = False\n",
    "    if hasattr(model, \"pos_embed\"):\n",
    "        model.pos_embed.requires_grad = False\n",
    "    if hasattr(model, \"mask_token\"):\n",
    "        model.mask_token.requires_grad = False\n",
    "    if hasattr(model, \"decoder_pos_embed\"):\n",
    "        model.decoder_pos_embed.requires_grad = False\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    total_params = trainable_params + non_trainable_params\n",
    "    \n",
    "    ratio = trainable_params / total_params if total_params > 0 else 0.0\n",
    "    ratio = round(ratio, 4)\n",
    "\n",
    "    return {\n",
    "        \"trainable\": trainable_params,\n",
    "        \"non_trainable\": non_trainable_params,\n",
    "        \"total\": total_params,\n",
    "        \"trainable_ratio %\": ratio*100\n",
    "    }\n",
    "\n",
    "\n",
    "pruned_model_full_blocks = torch.load('/home/ict317-3/Mohammad/Tiny-WFMs/our_pruned_models/pruned_task_models/sig_identification/pruned_ViT_for_radio_task.pth', weights_only=False)\n",
    "pruned_model_single_blocks = torch.load('/home/ict317-3/Mohammad/Tiny-WFMs/our_pruned_models/pruned_task_models_one_block/sig_identification/pruned_ViT_with_1_block_for_radio_task.pth', weights_only=False)\n",
    "pruned_model_single_blocks_single_head = torch.load('/home/ict317-3/Mohammad/Tiny-WFMs/our_pruned_models/pruned_task_models_one_block_one_layer_TH/sig_identification/pruned_ViT_with_1_block_for_radio_task.pth', weights_only=False)\n",
    "\n",
    "\n",
    "info_full = count_parameters(pruned_model_full_blocks)\n",
    "info_single = count_parameters(pruned_model_single_blocks)\n",
    "info_double_single = count_parameters(pruned_model_single_blocks_single_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce398d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trainable': 405524,\n",
       " 'non_trainable': 6402317,\n",
       " 'total': 6807841,\n",
       " 'trainable_ratio %': 5.96}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15584439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trainable': 405524,\n",
       " 'non_trainable': 1295690,\n",
       " 'total': 1701214,\n",
       " 'trainable_ratio %': 23.84}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a995280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trainable': 142868,\n",
       " 'non_trainable': 1295690,\n",
       " 'total': 1438558,\n",
       " 'trainable_ratio %': 9.93}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_double_single"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
