{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04f894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(globals()[\"_dh\"][0])\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0315897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ict317-3/Mohammad/Tiny-WFMs/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ict317-3/Mohammad/Tiny-WFMs/venv/lib/python3.12/site-packages/timm/optim/optim_factory.py:7: FutureWarning: Importing from timm.optim.optim_factory is deprecated, please import via timm.optim\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.optim\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import models_mae_hetero\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from gaussian_noise import AddGaussianNoise\n",
    "from dataset_classes.pretrain_csi_5g import CSI5G\n",
    "from dataset_classes.pretrain_csi_wifi import CSIWiFi\n",
    "from dataset_classes.spectrogram_images import SpectrogramImages\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import util.misc as misc\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "\n",
    "import models_mae_hetero\n",
    "from engine_pretrain_hetero import train_one_epoch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_pruning as tp\n",
    "\n",
    "import timm\n",
    "import copy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68bde72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model = MaskedAutoencoderViT(\n",
      "  (patch_embed): ModuleList(\n",
      "    (0): PatchEmbed(\n",
      "      (proj): Conv2d(1, 512, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (1): PatchEmbed(\n",
      "      (proj): Conv2d(3, 512, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (2): PatchEmbed(\n",
      "      (proj): Conv2d(4, 512, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_embed): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0-7): 8 x Block(\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (decoder_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_pred): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=768, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = 'mae_vit_small_patch16'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# define the model\n",
    "model = models_mae_hetero.__dict__[model](norm_pix_loss=False, in_chans=[1, 3, 4])\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "print(\"Model = %s\" % str(model_without_ddp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8cac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.patch_embed = model.patch_embed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c818978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.decoder_pred = model.decoder_pred[0]\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148d7f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dataset_classes.spectrogram_images.SpectrogramImages object at 0x7e8ba6dc0dd0> <dataset_classes.pretrain_csi_5g.CSI5G object at 0x7e8b6f00a5d0> <dataset_classes.pretrain_csi_wifi.CSIWiFi object at 0x7e8b6f00a690>\n"
     ]
    }
   ],
   "source": [
    "augmentation = True\n",
    "\n",
    "data_path = ['/home/ict317-3/Mohammad/Tiny-WFMs/pretraining_datasets/spectrogram_dataset',\n",
    "             '/home/ict317-3/Mohammad/Tiny-WFMs/pretraining_datasets/spectrogram_iqengine_dataset',\n",
    "             '/home/ict317-3/Mohammad/Tiny-WFMs/pretraining_datasets/5G_CFR',\n",
    "             '/home/ict317-3/Mohammad/Tiny-WFMs/pretraining_datasets/NTU-Fi-HumanID']\n",
    "\n",
    "log_dir = './output_dir'\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.functional.pil_to_tensor,\n",
    "        transforms.Lambda(lambda x: 10 * torch.log10(x + 1e-12)),\n",
    "        transforms.Lambda(lambda x: (x + 120) / (-0.5 + 120)),\n",
    "        transforms.Resize((224, 224), antialias=True,\n",
    "                          interpolation=InterpolationMode.BICUBIC),  # Resize\n",
    "        transforms.Normalize(mean=[0.451], std=[0.043])  # Normalize\n",
    "    ])\n",
    "\n",
    "dataset_train_one = SpectrogramImages(data_path[:-2], transform=transform_train)\n",
    "\n",
    "augment_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    AddGaussianNoise(mean=0.0, std=0.05)]\n",
    ")\n",
    "\n",
    "if augmentation:\n",
    "        dataset_train_two = CSI5G(data_path[-2], augment_transforms=augment_transforms)\n",
    "        dataset_train_three = CSIWiFi(data_path[-1], augment_transforms=augment_transforms)\n",
    "else:\n",
    "    dataset_trainlog_dir_two = CSI5G(data_path[-2])\n",
    "    dataset_train_three = CSIWiFi(data_path[-1])\n",
    "\n",
    "print(dataset_train_one, dataset_train_two, dataset_train_three)\n",
    "\n",
    "sampler_train_one = RandomSampler(dataset_train_one)\n",
    "sampler_train_two = RandomSampler(dataset_train_two)\n",
    "sampler_train_three = RandomSampler(dataset_train_three)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe15ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 10\n",
    "pin_mem = True\n",
    "csi_subsampling = False\n",
    "\n",
    "data_loader_train_one = DataLoader(\n",
    "        dataset_train_one, sampler=sampler_train_one,\n",
    "        batch_size= batch_size,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n",
    "\n",
    "if  csi_subsampling:\n",
    "    data_loader_train_two = DataLoader(\n",
    "        dataset_train_two, sampler=sampler_train_two,\n",
    "        batch_size= batch_size // 2,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n",
    "\n",
    "    data_loader_train_three = DataLoader(\n",
    "        dataset_train_three, sampler=sampler_train_three,\n",
    "        batch_size= batch_size // 2,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n",
    "else:\n",
    "    data_loader_train_two = DataLoader(\n",
    "        dataset_train_two, sampler=sampler_train_two,\n",
    "        batch_size= batch_size,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n",
    "\n",
    "    data_loader_train_three = DataLoader(\n",
    "        dataset_train_three, sampler=sampler_train_three,\n",
    "        batch_size= batch_size,\n",
    "        num_workers= num_workers,\n",
    "        pin_memory= pin_mem,\n",
    "        drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d39d75cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base lr: 1.00e-03\n",
      "actual lr: 6.25e-05\n",
      "accumulate grad iterations: 1\n",
      "effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "accum_iter = 1\n",
    "lr = None\n",
    "blr = 1e-3\n",
    "\n",
    "\n",
    "eff_batch_size =  batch_size *accum_iter\n",
    "    \n",
    "if lr is None:  # only base_lr is specified\n",
    "     lr = blr * eff_batch_size / 256\n",
    "\n",
    "print(\"base lr: %.2e\" % ( lr * 256 / eff_batch_size))\n",
    "print(\"actual lr: %.2e\" %  lr)\n",
    "\n",
    "print(\"accumulate grad iterations: %d\" %accum_iter)\n",
    "print(\"effective batch size: %d\" % eff_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d0239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 6.25e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 6.25e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ict317-3/Mohammad/Tiny-WFMs/util/misc.py:254: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_decay = 0.05\n",
    "\n",
    "# following timm: set wd as 0 for bias and norm layers\n",
    "param_groups = optim_factory.param_groups_weight_decay(model_without_ddp, weight_decay)\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=(0.9, 0.95))\n",
    "print(optimizer)\n",
    "loss_scaler = NativeScaler()\n",
    "\n",
    "ckpt_path = '/home/ict317-3/Mohammad/Tiny-WFMs/checkpoints/pretrained_all_data.pth'\n",
    "pretrained = torch.load(ckpt_path, map_location=device, weights_only=False)['model']\n",
    "model.load_state_dict(pretrained, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82257854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_baseline_performance(model, mask_ratio, kernel_size, dataset_id, device):\n",
    "\n",
    "#     accuracy = 0\n",
    "#     total_loss = 0\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     model = model.to(device)\n",
    "        \n",
    "#     data = {\n",
    "#         1: (dataset_train_one,   data_loader_train_one),\n",
    "#         2: (dataset_train_two,   data_loader_train_two),\n",
    "#         3: (dataset_train_three, data_loader_train_three),\n",
    "#     }\n",
    "#     assert dataset_id in data, f\"dataset_id must be 1, 2, or 3, got {dataset_id}\"\n",
    "\n",
    "\n",
    "#     with torch.no_grad():\n",
    "       \n",
    "#         for k, (images, _) in enumerate(tqdm(data[dataset_id][1], desc=\"Batches\", leave=False)):\n",
    "#             images = images.to(device)\n",
    "#             loss, reconstructed, mask = model(images, mask_ratio=mask_ratio / 100)\n",
    "#             images = torch.einsum('nchw->nhwc', images)\n",
    "#             reconstructed = torch.einsum('nchw->nhwc', model.unpatchify(reconstructed))\n",
    "#             mask = model.unpatchify(mask.unsqueeze(-1).repeat(1, 1, 16 ** 2 * 1))\n",
    "#             mask = torch.einsum('nchw->nhwc', mask)\n",
    "#             reconstructed = (1 - mask) * images + mask * reconstructed\n",
    "\n",
    "#             pooled_images = F.avg_pool2d(images.permute(0, 3, 1, 2), kernel_size=kernel_size, stride=kernel_size)\n",
    "#             pooled_reconstructed = F.avg_pool2d(reconstructed.permute(0, 3, 1, 2), kernel_size=kernel_size, stride=kernel_size)\n",
    "#             mu, std = torch.mean(pooled_images, dim=(1, 2, 3)), torch.std(pooled_images, dim=(1, 2, 3))\n",
    "#             threshold = mu + 0.5 * std\n",
    "#             threshold = threshold.view(-1, 1, 1, 1).repeat((1, 1, pooled_images.shape[2], pooled_images.shape[3]))\n",
    "#             pooled_images = pooled_images > threshold\n",
    "#             pooled_reconstructed = pooled_reconstructed > threshold\n",
    "\n",
    "#             accuracy += (pooled_images == pooled_reconstructed).sum().item()\n",
    "#             total_loss += loss.item() \n",
    "\n",
    "#     accuracy /= (len(data[dataset_id][0]) * (224 // kernel_size) ** 2)\n",
    "#     avg_loss = total_loss / len(data[dataset_id][1])\n",
    "\n",
    "#     print(accuracy)\n",
    "#     return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "158300f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_per_loader(model, device, data_loader, mask_ratio=75, kernel_size=3):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # map id -> (dataset, loader)\n",
    "    # data = {\n",
    "    #     1: data_loader_train_one,\n",
    "    #     2: data_loader_train_two,\n",
    "    #     3: data_loader_train_three,\n",
    "    # }\n",
    "    # assert dataset_id in data, f\"dataset_id must be 1, 2, or 3, got {dataset_id}\"\n",
    "\n",
    "    # loader = data[dataset_id]\n",
    "\n",
    "    total_equal = 0\n",
    "    total_cells_seen = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Try to infer patch size and channels for robust unpatchify of the mask\n",
    "    # Fallback to 16 and 1 if the model doesn't expose it\n",
    "    try:\n",
    "        p = getattr(model, \"patch_embed\").patch_size[0]  # e.g., 16\n",
    "    except Exception:\n",
    "        p = 16\n",
    "    # We'll infer channels per batch from the images\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(data_loader, desc=\"Batches\", leave=False):\n",
    "            images = images.to(device)  # NCHW expected here from your dataset pipeline\n",
    "\n",
    "            # forward\n",
    "            loss, reconstructed, mask = model(images, mask_ratio=mask_ratio / 100.0)\n",
    "\n",
    "            # Unpatchify outputs (assumes MAE-like shapes)\n",
    "            # reconstructed: (N, L, p*p*C), mask: (N, L)\n",
    "            # Infer channels from the reconstructed payload if possible\n",
    "            # C = (reconstructed.shape[-1] // (p * p)) if reconstructed.ndim == 3 else images.shape[1]\n",
    "            if reconstructed.ndim == 3:\n",
    "                C = reconstructed.shape[-1] // (p * p)\n",
    "            else:\n",
    "                C = images.shape[1]\n",
    "\n",
    "            reconstructed = model.unpatchify(reconstructed)  # -> (N, C, H, W)\n",
    "            # Broadcast mask from (N, L) to (N, p*p*C, L) before unpatchify\n",
    "            mask_expanded = mask.unsqueeze(-1).repeat(1, 1, p * p * C)\n",
    "            mask_img = model.unpatchify(mask_expanded)  # (N, C, H, W)\n",
    "\n",
    "            # Blend original and reconstructed only on masked regions\n",
    "            blended = (1 - mask_img) * images + mask_img * reconstructed  # all NCHW\n",
    "\n",
    "            # Pool down to grids\n",
    "            pooled_images = F.avg_pool2d(images, kernel_size=kernel_size, stride=kernel_size)\n",
    "            pooled_blended = F.avg_pool2d(blended, kernel_size=kernel_size, stride=kernel_size)\n",
    "\n",
    "            # Per-sample thresholds from original pooled images\n",
    "            # mean/std over (C,H,W)\n",
    "            mu = pooled_images.mean(dim=(1, 2, 3))\n",
    "            std = pooled_images.std(dim=(1, 2, 3), unbiased=False)  # avoid NaNs when only 1 cell\n",
    "            threshold = (mu + 0.5 * std).view(-1, 1, 1, 1)\n",
    "\n",
    "            # Binarize\n",
    "            bin_images = pooled_images > threshold\n",
    "            bin_blended = pooled_blended > threshold\n",
    "\n",
    "            # Count equal cells\n",
    "            equal_cells = (bin_images == bin_blended).sum().item()\n",
    "            total_equal += equal_cells\n",
    "\n",
    "            # Track how many cells we actually processed\n",
    "            n, c, h, w = bin_images.shape\n",
    "            total_cells_seen += n * c * h * w\n",
    "\n",
    "            # Accumulate batch loss\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "    # Safe aggregations\n",
    "    accuracy = total_equal / max(1, total_cells_seen)\n",
    "    avg_loss = total_loss / max(1, len(data_loader))\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aadf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_ratio = 75\n",
    "# kernel_size = 3\n",
    "# dataset_id = 3\n",
    "\n",
    "\n",
    "# loss = compute_loss_per_loader(model, data_loader_train_one, device)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb65f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baseline_performance(model, device, data_loaders):\n",
    "\n",
    "    overall_loss = sum(compute_loss_per_loader(model, device, loader)[0] for loader in data_loaders) / len(data_loaders)\n",
    "    \n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "502c5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_encoder_new(self, x, mask_ratio):\n",
    "#         # embed patches\n",
    "#         x = self.patch_embed(x)\n",
    "\n",
    "#         # add pos embed w/o cls token\n",
    "#         x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "#         # masking: length -> length * mask_ratio\n",
    "#         x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "#         # append cls token\n",
    "#         cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "#         cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "#         # apply Transformer blocks\n",
    "#         for blk in self.blocks:\n",
    "#             x = blk(x)\n",
    "#         x = self.norm(x)\n",
    "\n",
    "#         return x, mask, ids_restore\n",
    "\n",
    "\n",
    "# def forward_decoder_new(self, x, ids_restore):\n",
    "#         # embed tokens\n",
    "#         x = self.decoder_embed(x)\n",
    "\n",
    "#         # append mask tokens to sequence\n",
    "#         mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "#         x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "#         x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "#         x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "#         # add pos embed\n",
    "#         x = x + self.decoder_pos_embed\n",
    "\n",
    "#         # apply Transformer blocks\n",
    "#         for blk in self.decoder_blocks:\n",
    "#             x = blk(x)\n",
    "#         x = self.decoder_norm(x)\n",
    "\n",
    "#         # predictor projection\n",
    "#         x = self.decoder_pred(x)\n",
    "\n",
    "#         # remove cls token\n",
    "#         x = x[:, 1:, :]\n",
    "\n",
    "#         return x\n",
    "\n",
    "# def forward_new(self, imgs, mask_ratio=0.75):\n",
    "#     latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)  # <- renamed\n",
    "#     pred = self.forward_decoder(latent, ids_restore)                    # <- renamed\n",
    "#     loss = self.forward_loss(imgs, pred, mask)\n",
    "#     return loss, pred, mask\n",
    "\n",
    "# import types\n",
    "\n",
    "# model.forward_encoder = types.MethodType(forward_encoder_new, model)\n",
    "# model.forward_decoder = types.MethodType(forward_decoder_new, model)\n",
    "# model.forward          = types.MethodType(forward_new, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b801091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loaders = [data_loader_train_one, data_loader_train_two, data_loader_train_three]\n",
    "\n",
    "# loss = compute_baseline_performance(model, 'cuda', data_loaders)\n",
    "\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07cf49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_blocks(model):\n",
    "    ignored_blocks = []\n",
    "    model_blocks = []\n",
    "    num_heads = {}\n",
    "    bottleneck = False\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, timm.models.vision_transformer.Attention):\n",
    "            num_heads[m.qkv] = m.num_heads\n",
    "        if bottleneck and isinstance(m, timm.models.vision_transformer.Mlp):\n",
    "            ignored_blocks.append(m.fc2)\n",
    "\n",
    "    for name, layer in model.named_children():\n",
    "        if name != 'blocks':\n",
    "            if len(list(layer.children())) > 0:\n",
    "                ignored_blocks.extend(layer.children())\n",
    "            else:\n",
    "                ignored_blocks.append(layer)\n",
    "        else:\n",
    "            if len(list(layer.children())) > 0:\n",
    "                model_blocks.extend(layer.children())\n",
    "            else:\n",
    "                model_blocks.append(layer)\n",
    "\n",
    "    return model_blocks, ignored_blocks, num_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba79728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine_pretrain_hetero import RoundRobinLoader\n",
    "\n",
    "def selective_block_pruning(trained_model, device, pruning_ratios, prune_method, data_loaders, mask_ratio):\n",
    "    model = copy.deepcopy(trained_model)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model_blocks, ignored_blocks, num_heads = get_blocks(model)\n",
    "    model.to(device)\n",
    "\n",
    "    pruning_info = {\n",
    "        i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio}\n",
    "        for i, ratio in enumerate(pruning_ratios)\n",
    "    }\n",
    "\n",
    "    if prune_method == 'channel_pruning_Taylor_importance':\n",
    "        imp = tp.importance.GroupTaylorImportance()\n",
    "\n",
    "        combined_iter = RoundRobinLoader(data_loaders)\n",
    "\n",
    "        if isinstance(imp, tp.importance.GroupTaylorImportance):\n",
    "            model.zero_grad()\n",
    "            model.train(True)\n",
    "\n",
    "            print(\"Accumulating gradients for pruning...\")\n",
    "            for data_iter_step, (key, samples, labels) in enumerate(combined_iter):\n",
    "                samples, labels = samples.to(device), labels.to(device)\n",
    "                if data_iter_step >= 60: # 20 samples of each dataloader for now\n",
    "                    break\n",
    "                loss, _, _ = model(samples, mask_ratio=mask_ratio / 100)\n",
    "                loss.backward()\n",
    "\n",
    "        original_macs, original_nparams = tp.utils.count_ops_and_params(model, samples)\n",
    "\n",
    "        for i, info in pruning_info.items():\n",
    "            pruning_ratio = info[\"pruning_ratio\"]\n",
    "            if pruning_ratio == 0:\n",
    "                continue\n",
    "\n",
    "            ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "            combined_ignored_layers = ignored_blocks + ignored_layers_block\n",
    "            # print('combined_ignored_layers:   ', combined_ignored_layers)\n",
    "            \n",
    "            count = 0\n",
    "            while True:\n",
    "                pruner = tp.pruner.MetaPruner(\n",
    "                    model,\n",
    "                    example_inputs=samples,\n",
    "                    importance=imp,\n",
    "                    pruning_ratio=pruning_ratio,\n",
    "                    ignored_layers=combined_ignored_layers,\n",
    "                    num_heads=num_heads,\n",
    "                    prune_num_heads=False,\n",
    "                    prune_head_dims=True\n",
    "                )\n",
    "                for g in pruner.step(interactive=True):\n",
    "                    g.prune()\n",
    "\n",
    "                # print(f'A pruning process has been performed here with a {pruning_ratio} pruning ratio ...  ')\n",
    "                \n",
    "                for m in model.modules():\n",
    "                    if isinstance(m, timm.models.vision_transformer.Attention):\n",
    "                        m.num_heads = pruner.num_heads[m.qkv]\n",
    "                        m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n",
    "\n",
    "                macs, nparams = tp.utils.count_ops_and_params(model, samples)\n",
    "\n",
    "                if original_nparams - nparams == 0:\n",
    "                    count += 1\n",
    "                    if count == 1:\n",
    "                        pruning_ratio = 0.5\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                original_nparams = nparams\n",
    "\n",
    "        del samples, labels\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return model, macs, nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a3a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_analysis_with_contributions(original_model, device, metric, measure_performance, data_loaders, mask_ratio=75):\n",
    "    model_blocks, _, _ = get_blocks(original_model)\n",
    "    blocks_number = len(model_blocks)\n",
    "\n",
    "    total_block_performance = [0.0 for _ in range(blocks_number)]\n",
    "    params_reduction = []\n",
    "    macs_reduction = []\n",
    "\n",
    "    original_model.to(device)\n",
    "\n",
    "    print(f\"Computing baseline {metric} without block replacement...\")\n",
    "    baseline_performance = measure_performance(original_model, device, data_loaders)\n",
    "    print(f\"Baseline {metric}: {baseline_performance}\")\n",
    "\n",
    "    example_inputs = next(iter(data_loaders[0]))[0].to(device)\n",
    "    original_macs, original_nparams = tp.utils.count_ops_and_params(original_model, example_inputs)\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "        print(f\"Replacing block {block_idx}\")\n",
    "        pruning_ratios = (np.eye(blocks_number) * 0.85)[block_idx]\n",
    "        pruned_model, macs, nparams = selective_block_pruning(\n",
    "            original_model, device, pruning_ratios, 'channel_pruning_Taylor_importance', data_loaders, mask_ratio\n",
    "        )\n",
    "\n",
    "        params_reduction.append((original_nparams - nparams)/original_nparams * 100)\n",
    "        macs_reduction.append((original_macs - macs) / original_macs * 100)\n",
    "\n",
    "        pruned_model.to(device)\n",
    "        block_performance = measure_performance(pruned_model, device, data_loaders)\n",
    "        total_block_performance[block_idx] = block_performance\n",
    "        print(f'The {metric} after pruning this block is: {block_performance}')\n",
    "\n",
    "    total_degradation_in_performance = 0.0\n",
    "    block_degradation = []\n",
    "    total_params_reduction = 0.0\n",
    "    total_macs_reduction = 0.0\n",
    "\n",
    "    for block_idx in range(blocks_number):\n",
    "        degradation = np.abs(total_block_performance[block_idx] - baseline_performance)\n",
    "        print(f\"Degradation in {metric} is: {degradation}\")\n",
    "        block_degradation.append(degradation)\n",
    "        total_degradation_in_performance += degradation\n",
    "        total_params_reduction += params_reduction[block_idx]\n",
    "        total_macs_reduction += macs_reduction[block_idx]\n",
    "\n",
    "    relative_contributions = []\n",
    "    weighted_importance_scores = []\n",
    "\n",
    "    print(f\"\\nRelative contribution of each block to total {metric} degradation and parameter reduction:\")\n",
    "    for block_idx in range(blocks_number):\n",
    "        rel_perf = (block_degradation[block_idx] / total_degradation_in_performance) * 100\n",
    "        rel_params = 100 - params_reduction[block_idx]\n",
    "        rel_macs = 100 - macs_reduction[block_idx]\n",
    "\n",
    "        weighted_importance = (0.7 * rel_perf) + (0.2 * rel_params) + (0.1 * rel_macs)\n",
    "        print(f'Block {block_idx} contributes {rel_perf:.2f}% to the total degradation in {metric} and reduces {params_reduction[block_idx]:.2f}% of parameters.')\n",
    "        print(f'Weighted importance score for Block {block_idx}: {weighted_importance:.2f}')\n",
    "\n",
    "        relative_contributions.append(rel_perf)\n",
    "        weighted_importance_scores.append(weighted_importance)\n",
    "\n",
    "    return weighted_importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    \"\"\"https://github.com/huggingface/pytorch-image-models/blob/054c763fcaa7d241564439ae05fbe919ed85e614/timm/models/vision_transformer.py#L79\"\"\"\n",
    "    B, N, C = x.shape\n",
    "    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv.unbind(0)\n",
    "    q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "    if self.fused_attn:\n",
    "        x = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            dropout_p=self.attn_drop.p,\n",
    "        )\n",
    "    else:\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = attn @ v\n",
    "\n",
    "    x = x.transpose(1, 2).reshape(B, N, -1) # original implementation: x = x.transpose(1, 2).reshape(B, N, C)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "    return x\n",
    "\n",
    "# Override attention forward\n",
    "for m in model.modules():\n",
    "    if isinstance(m, timm.models.vision_transformer.Attention):\n",
    "        m.forward = forward.__get__(m, timm.models.vision_transformer.Attention)\n",
    "        \n",
    "# Analyze contributions\n",
    "relative_contribution = perplexity_analysis_with_contributions(\n",
    "    model,\n",
    "    device=device,\n",
    "    metric='loss',\n",
    "    measure_performance=compute_baseline_performance,\n",
    "    data_loaders=[data_loader_train_one, data_loader_train_two, data_loader_train_three]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04022fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_contribution = [np.float64(68.08877057598382),\n",
    " np.float64(37.40626909822759),\n",
    " np.float64(32.281119143100256),\n",
    " np.float64(30.637165097452176),\n",
    " np.float64(30.020183495139563),\n",
    " np.float64(29.474453270385183),\n",
    " np.float64(29.440264146737952),\n",
    " np.float64(29.357529826182528),\n",
    " np.float64(29.954904338606084),\n",
    " np.float64(29.66567846836903),\n",
    " np.float64(30.0643648149045),\n",
    " np.float64(31.607907928914102)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ae3576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [np.float64(67.84490239830508),\n",
    "#  np.float64(37.50249339853453),\n",
    "#  np.float64(32.31163056372445),\n",
    "#  np.float64(30.57015590482878),\n",
    "#  np.float64(29.96990099259839),\n",
    "#  np.float64(29.514538402508663),\n",
    "#  np.float64(29.43987769508422),\n",
    "#  np.float64(29.401976151389682),\n",
    "#  np.float64(29.970814767218833),\n",
    "#  np.float64(29.648641127295463),\n",
    "#  np.float64(30.145338707694115),\n",
    "#  np.float64(31.678340094820577)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd73c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pruning_ratios(contributions, max_pruning_ratio=0.9, k=5):\n",
    "    \"\"\"\n",
    "    Calculate pruning ratios based on intense nonlinear scaling (exponential decay) of the relative contributions.\n",
    "\n",
    "    Parameters:\n",
    "    - contributions (list): List of relative contributions (in percentages) of each block to total loss increase.\n",
    "    - max_pruning_ratio (float): Maximum pruning ratio to be assigned to the least important layer. Default is 0.9.\n",
    "    - k (int): Factor controlling the intensity of the scaling (larger k makes the ratio more intense).\n",
    "\n",
    "    Returns:\n",
    "    - pruning_ratios (list): List of pruning ratios for each block.\n",
    "    \"\"\"\n",
    "    # Normalize the contributions to get values between 0 and 1\n",
    "    total_contribution = sum(contributions)\n",
    "    normalized_contributions = [contribution / total_contribution for contribution in contributions]\n",
    "\n",
    "    # Apply exponential decay to magnify the effect for less important blocks\n",
    "    pruning_factors = [np.exp(-k * nc) for nc in normalized_contributions]\n",
    "\n",
    "    # Normalize the pruning factors so they stay within the max pruning ratio\n",
    "    max_factor = max(pruning_factors)\n",
    "    normalized_factors = [pf / max_factor for pf in pruning_factors]\n",
    "\n",
    "    # Scale by the maximum pruning ratio\n",
    "    pruning_ratios = [max_pruning_ratio * nf for nf in normalized_factors]\n",
    "\n",
    "    pruning_ratios = [round(num, 2) for num in pruning_ratios]\n",
    "\n",
    "    return pruning_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e69ce19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(trained_model, device, pruning_ratios, prune_method, data_loaders, mask_ratio=75):\n",
    "    model = copy.deepcopy(trained_model)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model_blocks, ignored_blocks, num_heads = get_blocks(model)\n",
    "    model.to(device)\n",
    "\n",
    "    pruning_info = {\n",
    "        i: {\"block\": model_blocks[i], \"pruning_ratio\": ratio}\n",
    "        for i, ratio in enumerate(pruning_ratios)\n",
    "    }\n",
    "\n",
    "    if prune_method == 'channel_pruning_Taylor_importance':\n",
    "        imp = tp.importance.GroupTaylorImportance()\n",
    "\n",
    "        combined_iter = RoundRobinLoader(data_loaders)\n",
    "\n",
    "        if isinstance(imp, tp.importance.GroupTaylorImportance):\n",
    "            model.zero_grad()\n",
    "            model.train(True)\n",
    "\n",
    "            print(\"Accumulating gradients for pruning...\")\n",
    "            for data_iter_step, (key, samples, labels) in enumerate(combined_iter):\n",
    "                samples, labels = samples.to(device), labels.to(device)\n",
    "                if data_iter_step >= 60: # 20 samples of each dataloader for now\n",
    "                    break\n",
    "                loss, _, _ = model(samples, mask_ratio=mask_ratio / 100)\n",
    "                loss.backward()\n",
    "\n",
    "        original_macs, original_nparams = tp.utils.count_ops_and_params(model, samples)\n",
    "\n",
    "        for i, info in pruning_info.items():\n",
    "            pruning_ratio = info[\"pruning_ratio\"]\n",
    "   \n",
    "\n",
    "            ignored_layers_block = [pruning_info[j][\"block\"] for j in range(len(pruning_info)) if j != i]\n",
    "            combined_ignored_layers = ignored_blocks + ignored_layers_block\n",
    "            # print('combined_ignored_layers:   ', combined_ignored_layers)\n",
    "            \n",
    "            print(f\"Pruning block {i} with pruning ratio: {pruning_ratio}\")\n",
    "\n",
    "            pruner = tp.pruner.MetaPruner(\n",
    "                model,\n",
    "                example_inputs=samples,\n",
    "                importance=imp,\n",
    "                pruning_ratio=pruning_ratio,\n",
    "                ignored_layers=combined_ignored_layers,\n",
    "                num_heads=num_heads,\n",
    "                prune_num_heads=False,\n",
    "                prune_head_dims=True\n",
    "            )\n",
    "            for g in pruner.step(interactive=True):\n",
    "                g.prune()\n",
    "\n",
    "            # print(f'A pruning process has been performed here with a {pruning_ratio} pruning ratio ...  ')\n",
    "            \n",
    "            for m in model.modules():\n",
    "                if isinstance(m, timm.models.vision_transformer.Attention):\n",
    "                    m.num_heads = pruner.num_heads[m.qkv]\n",
    "                    m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n",
    "\n",
    "            macs, nparams = tp.utils.count_ops_and_params(model, samples)\n",
    "\n",
    "            print(f\"MACs: {macs / 1e9:.2f} G, #Params: {nparams / 1e3:.2f} K\")\n",
    "            print(f\"Parameter reduction: {((original_nparams - nparams) / original_macs * 100):.2f}%\")\n",
    "            print(f\"MACs reduction: {((original_macs - macs) / original_macs * 100):.2f}%\")\n",
    "        \n",
    "\n",
    "                \n",
    "        del samples, labels\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return model, macs, nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5acb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 Pruning Ratio: 0.6200\n",
      "Block 1 Pruning Ratio: 0.9000\n",
      "Block 2 Pruning Ratio: 0.9600\n",
      "Block 3 Pruning Ratio: 0.9700\n",
      "Block 4 Pruning Ratio: 0.9800\n",
      "Block 5 Pruning Ratio: 0.9900\n",
      "Block 6 Pruning Ratio: 0.9900\n",
      "Block 7 Pruning Ratio: 0.9900\n",
      "Block 8 Pruning Ratio: 0.9800\n",
      "Block 9 Pruning Ratio: 0.9900\n",
      "Block 10 Pruning Ratio: 0.9800\n",
      "Block 11 Pruning Ratio: 0.9600\n"
     ]
    }
   ],
   "source": [
    "max_pruning_ratio = 0.99 # Maximum pruning ratio (99%) \n",
    "k = 5 # Controls the intensity of the scaling\n",
    "\n",
    "pruning_ratios = calculate_pruning_ratios(relative_contribution, max_pruning_ratio, k)\n",
    "\n",
    "# Print the pruning ratios for each block\n",
    "for i, ratio in enumerate(pruning_ratios):\n",
    "    print(f\"Block {i} Pruning Ratio: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64639093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating gradients for pruning...\n",
      "Pruning block 0 with pruning ratio: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ict317-3/Mohammad/Tiny-WFMs/venv/lib/python3.12/site-packages/torch_pruning/dependency.py:699: UserWarning: Unwrapped parameters detected: ['pos_embed', 'decoder_pos_embed', 'mask_token', 'cls_token'].\n",
      " Torch-Pruning will prune the last non-singleton dimension of these parameters. If you wish to change this behavior, please provide an unwrapped_parameters argument.\n",
      "  warnings.warn(warning_str)\n",
      "/home/ict317-3/Mohammad/Tiny-WFMs/venv/lib/python3.12/site-packages/torch_pruning/dependency.py:839: UserWarning: [Warning] Unknown operation None encountered, which will be handled as an element-wise op\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs: 3.27 G, #Params: 44049.99 K\n",
      "Parameter reduction: 0.06%\n",
      "MACs reduction: 3.04%\n",
      "Pruning block 1 with pruning ratio: 0.9\n",
      "MACs: 3.13 G, #Params: 41208.23 K\n",
      "Parameter reduction: 0.14%\n",
      "MACs reduction: 7.35%\n",
      "Pruning block 2 with pruning ratio: 0.96\n",
      "MACs: 2.97 G, #Params: 38174.76 K\n",
      "Parameter reduction: 0.23%\n",
      "MACs reduction: 11.93%\n",
      "Pruning block 3 with pruning ratio: 0.97\n",
      "MACs: 2.82 G, #Params: 35104.38 K\n",
      "Parameter reduction: 0.32%\n",
      "MACs reduction: 16.56%\n",
      "Pruning block 4 with pruning ratio: 0.98\n",
      "MACs: 2.66 G, #Params: 32012.48 K\n",
      "Parameter reduction: 0.41%\n",
      "MACs reduction: 21.22%\n",
      "Pruning block 5 with pruning ratio: 0.99\n",
      "MACs: 2.56 G, #Params: 29933.78 K\n",
      "Parameter reduction: 0.48%\n",
      "MACs reduction: 24.30%\n",
      "Pruning block 6 with pruning ratio: 0.99\n",
      "MACs: 2.45 G, #Params: 27855.08 K\n",
      "Parameter reduction: 0.54%\n",
      "MACs reduction: 27.38%\n",
      "Pruning block 7 with pruning ratio: 0.99\n",
      "MACs: 2.35 G, #Params: 25776.38 K\n",
      "Parameter reduction: 0.60%\n",
      "MACs reduction: 30.45%\n",
      "Pruning block 8 with pruning ratio: 0.98\n",
      "MACs: 2.19 G, #Params: 22684.47 K\n",
      "Parameter reduction: 0.69%\n",
      "MACs reduction: 35.11%\n",
      "Pruning block 9 with pruning ratio: 0.99\n",
      "MACs: 2.09 G, #Params: 20605.77 K\n",
      "Parameter reduction: 0.75%\n",
      "MACs reduction: 38.19%\n",
      "Pruning block 10 with pruning ratio: 0.98\n",
      "MACs: 1.93 G, #Params: 17513.87 K\n",
      "Parameter reduction: 0.84%\n",
      "MACs reduction: 42.85%\n",
      "Pruning block 11 with pruning ratio: 0.96\n",
      "MACs: 1.77 G, #Params: 14480.40 K\n",
      "Parameter reduction: 0.93%\n",
      "MACs reduction: 47.43%\n"
     ]
    }
   ],
   "source": [
    "data_loaders = [data_loader_train_one, data_loader_train_two, data_loader_train_three]\n",
    "pruned_model, macs, nparams = prune_model(model, device, pruning_ratios, 'channel_pruning_Taylor_importance', data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d7887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.404765754683404"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders = [data_loader_train_one, data_loader_train_two, data_loader_train_three]\n",
    "\n",
    "loss = compute_baseline_performance(pruned_model, 'cuda', data_loaders)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4866cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs: 3.376390688 G -> 1.7748810898125 G, #Params: 46.008064 M -> 14.480397 M\n",
      "Overall parameter reduction: 68.53%\n",
      "Overall MACs reduction: 47.43%\n"
     ]
    }
   ],
   "source": [
    "images = next(iter(data_loaders[0]))[0].to(device)\n",
    "model.to(device)\n",
    "\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, images)\n",
    "macs, nparams = tp.utils.count_ops_and_params(pruned_model, images)\n",
    "\n",
    "print(f\"MACs: {base_macs/1e9} G -> {macs/1e9} G, #Params: {base_nparams/1e6} M -> {nparams/1e6} M\")\n",
    "p_ratio = ((base_nparams - nparams) / base_nparams * 100)\n",
    "\n",
    "print(f\"Overall parameter reduction: {(p_ratio):.2f}%\")\n",
    "print(f\"Overall MACs reduction: {((base_macs - macs) / base_macs * 100):.2f}%\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3306634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model, f'our_pruned_models/pruned_autoencoder/Vit_pruned_{(p_ratio):.2f}%.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c4c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in model_a but missing in model_b: {'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.7.norm1.weight', 'decoder_pred.0.weight', 'decoder_embed.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.5.mlp.fc1.bias', 'patch_embed.2.proj.bias', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_embed.weight', 'decoder_pos_embed', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.norm2.weight', 'patch_embed.1.proj.bias', 'decoder_pred.1.weight', 'decoder_pred.2.weight', 'decoder_norm.bias', 'patch_embed.2.proj.weight', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.6.norm1.bias', 'patch_embed.0.proj.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.0.norm2.weight', 'mask_token', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.0.mlp.fc1.weight', 'patch_embed.0.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.7.attn.proj.bias', 'decoder_pred.2.bias', 'decoder_blocks.0.attn.proj.weight', 'patch_embed.1.proj.weight', 'decoder_norm.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.3.norm1.weight', 'decoder_pred.1.bias', 'decoder_blocks.2.attn.qkv.bias', 'decoder_pred.0.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.5.mlp.fc2.bias'}\n",
      "Parameters in model_b but missing in model_a: {'head.3.bias', 'patch_embed.proj.bias', 'head.3.weight', 'head.0.bias', 'patch_embed.proj.weight', 'head.0.weight'}\n",
      "[Value mismatch] pos_embed | Max diff: 1.08e+00\n",
      "[Value mismatch] cls_token | Max diff: 6.78e-02\n",
      "[Value mismatch] norm.weight | Max diff: 6.71e-01\n",
      "[Value mismatch] norm.bias | Max diff: 1.37e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pos_embed', 1.0822968482971191),\n",
       " ('cls_token', 0.06781171262264252),\n",
       " ('norm.weight', 0.6712888479232788),\n",
       " ('norm.bias', 0.13735242187976837)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_model_weights(model_a, model_b, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare the weights of two PyTorch models.\n",
    "\n",
    "    Args:\n",
    "        model_a (torch.nn.Module): First model.\n",
    "        model_b (torch.nn.Module): Second model.\n",
    "        verbose (bool): If True, prints differences.\n",
    "\n",
    "    Returns:\n",
    "        differences (list): List of tuples (param_name, max_diff) where differences were found.\n",
    "    \"\"\"\n",
    "    differences = []\n",
    "\n",
    "    state_dict_a = model_a.state_dict()\n",
    "    state_dict_b = model_b.state_dict()\n",
    "\n",
    "    keys_a = set(state_dict_a.keys())\n",
    "    keys_b = set(state_dict_b.keys())\n",
    "\n",
    "    # Check for missing parameters\n",
    "    if keys_a != keys_b:\n",
    "        missing_in_b = keys_a - keys_b\n",
    "        missing_in_a = keys_b - keys_a\n",
    "\n",
    "        if missing_in_b:\n",
    "            print(f\"Parameters in model_a but missing in model_b: {missing_in_b}\")\n",
    "        if missing_in_a:\n",
    "            print(f\"Parameters in model_b but missing in model_a: {missing_in_a}\")\n",
    "\n",
    "        # Only compare common keys\n",
    "        common_keys = keys_a.intersection(keys_b)\n",
    "    else:\n",
    "        common_keys = keys_a\n",
    "\n",
    "    # Compare weights\n",
    "    for key in common_keys:\n",
    "        tensor_a = state_dict_a[key]\n",
    "        tensor_b = state_dict_b[key]\n",
    "\n",
    "        if not torch.allclose(tensor_a, tensor_b, atol=1e-6):\n",
    "            max_diff = (tensor_a - tensor_b).abs().max().item()\n",
    "            differences.append((key, max_diff))\n",
    "            if verbose:\n",
    "                print(f\"[Value mismatch] {key} | Max diff: {max_diff:.2e}\")\n",
    "\n",
    "    if not differences and verbose:\n",
    "        print(\" Models are identical (within tolerance)!\")\n",
    "\n",
    "    return differences\n",
    "\n",
    "\n",
    "original_model = torch.load('/home/ict317-3/Mohammad/Tiny-WFMs/our_pruned_models/pruned_autoencoder/Vit_pruned_68.53%.pth', weights_only=False)\n",
    "pruned_model = torch.load('/home/ict317-3/Mohammad/Tiny-WFMs/pruned_results/sig_identification/best_model.pth', weights_only=False)\n",
    "\n",
    "original_model.to('cuda')\n",
    "pruned_model.to('cuda')\n",
    "compare_model_weights(original_model, pruned_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
